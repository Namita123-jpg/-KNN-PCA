{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "Answer: K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric and instance-based learning algorithm, meaning it makes predictions based on the entire training dataset rather than learning explicit parameters.\n",
        "\n",
        "How KNN Works (General Process):\n",
        "\n",
        "Store the training data.\n",
        "\n",
        "When a new data point (query) needs to be predicted:\n",
        "\n",
        "Measure the distance between the query point and all training points (commonly using Euclidean distance).\n",
        "\n",
        "Identify the ‚Äòk‚Äô closest data points (neighbors).\n",
        "\n",
        "Use these neighbors to make a prediction."
      ],
      "metadata": {
        "id": "OWDvMiI4U7CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "Answer: The Curse of Dimensionality refers to various issues and challenges that arise when analyzing or organizing data in high-dimensional spaces (i.e., when the number of features or dimensions increases).\n",
        "\n",
        "As dimensions increase:\n",
        "\n",
        "The volume of the space increases exponentially.\n",
        "\n",
        "Data points become sparse.\n",
        "\n",
        "Distances between points become less meaningful or less distinguishable.\n",
        "\n",
        "Algorithms that rely on distance measures (like KNN) often perform poorly.\n",
        "\n",
        "How It Affects KNN Performance:\n",
        "\n",
        "K-Nearest Neighbors (KNN) relies heavily on distance metrics to find the nearest neighbors. In high-dimensional spaces, these distances become unreliable due to:\n",
        "\n",
        "1. Distance Convergence:\n",
        "\n",
        "In high dimensions, the difference between the nearest and farthest neighbor becomes very small.\n",
        "\n",
        "This makes it difficult for KNN to distinguish between close and far points.\n",
        "\n",
        "As a result, KNN may include irrelevant or distant neighbors in its predictions, reducing accuracy.\n",
        "\n",
        "2. Increased Noise and Irrelevance:\n",
        "\n",
        "High-dimensional data often includes many irrelevant or redundant features.\n",
        "\n",
        "These irrelevant features can distort distance calculations, leading to misleading neighbor selection.\n",
        "\n",
        "3. Sparsity of Data:\n",
        "\n",
        "The data becomes sparse, meaning each data point is far from every other point.\n",
        "\n",
        "KNN requires a dense neighborhood to find good neighbors ‚Äî but in high dimensions, such neighborhoods often don‚Äôt exist.\n",
        "\n",
        "4. Computational Complexity:\n",
        "\n",
        "As the number of dimensions increases, so does the computational cost of calculating distances for all training samples.\n",
        "\n",
        "This makes KNN increasingly slow and inefficient.\n",
        "\n",
        "üõ†Ô∏è How to Mitigate the Curse of Dimensionality in KNN:\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        "Choose only the most relevant features using techniques like correlation analysis, recursive feature elimination (RFE), or LASSO.\n",
        "\n",
        "Dimensionality Reduction:\n",
        "\n",
        "Use methods like PCA (Principal Component Analysis) or t-SNE to reduce the number of dimensions while retaining important information.\n",
        "\n",
        "Distance Metric Tuning:\n",
        "\n",
        "Consider alternate metrics like Manhattan distance or Mahalanobis distance which might perform better in some high-dimensional cases.\n",
        "\n",
        "Normalization/Standardization:\n",
        "\n",
        "Scale all features so that no single feature dominates the distance calculation."
      ],
      "metadata": {
        "id": "RE6I8VmoVPVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "Answer: Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining as much variability (information) as possible.\n",
        "\n",
        "It transforms the original features into a new set of uncorrelated variables called principal components, which are ordered by the amount of variance they capture from the data.\n",
        "\n",
        "| Aspect               | PCA (Dimensionality Reduction)         | Feature Selection                             |\n",
        "| -------------------- | -------------------------------------- | --------------------------------------------- |\n",
        "| **What it does**     | Transforms features into new ones      | Selects a subset of original features         |\n",
        "| **Output features**  | New, uncorrelated features (PCs)       | Original features only                        |\n",
        "| **Interpretability** | Low ‚Äì PCs are combinations of features | High ‚Äì original features are preserved        |\n",
        "| **Goal**             | Maximize variance retained             | Retain most informative/or relevant features  |\n",
        "| **Technique Type**   | **Feature extraction**                 | **Feature selection**                         |\n",
        "| **Example methods**  | PCA, t-SNE, LDA                        | Filter (e.g., Chi-squared), Wrapper, Embedded |\n"
      ],
      "metadata": {
        "id": "R2OKi9FEVln0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer :-  What are Eigenvalues and Eigenvectors?\n",
        "\n",
        "In the context of Principal Component Analysis (PCA):\n",
        "\n",
        "An eigenvector represents a direction or axis in the feature space.\n",
        "\n",
        "An eigenvalue represents the amount of variance (or information) captured in that direction.\n",
        "\n",
        "Think of eigenvectors as directions of maximum variance in your data, and eigenvalues as how important those directions are.\n",
        "\n",
        "Why Are They Important in PCA?\n",
        "üîπ 1. Finding Principal Components:\n",
        "\n",
        "PCA computes the covariance matrix of the data.\n",
        "\n",
        "Then it finds the eigenvectors and eigenvalues of this matrix.\n",
        "\n",
        "Each eigenvector becomes a principal component direction.\n",
        "\n",
        "Each eigenvalue tells you how much variance that component captures.\n",
        "\n",
        "üîπ 2. Ordering Importance:\n",
        "\n",
        "PCA sorts components based on eigenvalues (from largest to smallest).\n",
        "\n",
        "The top components (with largest eigenvalues) are kept for dimensionality reduction.\n",
        "\n",
        "üîπ 3. Dimensionality Reduction:\n",
        "\n",
        "By keeping only the top k eigenvectors (with highest eigenvalues), you reduce the number of features while preserving most of the variance"
      ],
      "metadata": {
        "id": "VapfwmClV6Om"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdjHNmLOWPB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Answer: Using Principal Component Analysis (PCA) before applying K-Nearest Neighbors (KNN) can significantly improve model performance, especially on high-dimensional datasets. They complement each other by solving different challenges:\n",
        "\n",
        "PCA\tReduces feature space, removes noise\n",
        "\n",
        "KNN\tClassifies/regresses based on distances\n",
        "\n",
        "| Component | Role in Pipeline                                             |\n",
        "| --------- | ------------------------------------------------------------ |\n",
        "| PCA       | Reduces dimensions, denoises data                            |\n",
        "| KNN       | Predicts based on nearest neighbors using improved distances |\n"
      ],
      "metadata": {
        "id": "fnt2_UWlWWbW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cICCu0WW-lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "VHEnqQWEXdlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# --- KNN without feature scaling ---\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --- KNN with feature scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Print results ---\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsdMfyq3Xei8",
        "outputId": "56572f44-a245-4f1a-f1c9-d4908f36c9c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8056\n",
            "Accuracy with scaling:    0.9722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "GxiUJbo6X-sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Display each principal component's contribution\n",
        "for i, var_ratio in enumerate(explained_variance):\n",
        "    print(f\"Principal Component {i+1}: {var_ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cR_zyfQYBuw",
        "outputId": "468873c1-4db7-4bd3-d3a3-ba98a615027d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nr0Nkc25YVec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "VmyV8EkGYYGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ---- KNN on original scaled data ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ---- PCA Transformation (top 2 components) ----\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# ---- KNN on PCA-reduced data ----\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# ---- Print results ----\n",
        "print(f\"Accuracy on original dataset (scaled): {accuracy_original:.4f}\")\n",
        "print(f\"Accuracy on PCA-reduced dataset (2 components): {accuracy_pca:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU8jtmY0YY06",
        "outputId": "4b5ff10b-b13e-4e56-fdf4-82bd9c6442fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset (scaled): 0.9722\n",
            "Accuracy on PCA-reduced dataset (2 components): 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "J79bpb_aYmRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---- KNN with Euclidean distance (default) ----\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# ---- KNN with Manhattan distance ----\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# ---- Print Results ----\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-SvTfg3YnfI",
        "outputId": "4e85a55a-b2ad-4561-9595-1ee9e888dbc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9722\n",
            "Accuracy with Manhattan distance: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "‚óè Use PCA to reduce dimensionality\n",
        "‚óè Decide how many components to keep\n",
        "‚óè Use KNN for classification post-dimensionality reduction\n",
        "‚óè Evaluate the model\n",
        "‚óè Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "aeaWwJF8Y5qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Simulate high-dimensional gene expression data\n",
        "X, y = make_classification(n_samples=100, n_features=1000, n_informative=50,\n",
        "                           n_redundant=950, n_classes=3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (retain 95% of variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original features: {X.shape[1]}\")\n",
        "print(f\"PCA-reduced features: {X_pca.shape[1]}\")\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Evaluate using Stratified K-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(knn, X_pca, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation results\n",
        "print(f\"\\nCross-validated Accuracy: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
        "\n",
        "# Final model evaluation on full data (optional)\n",
        "knn.fit(X_pca, y)\n",
        "y_pred = knn.predict(X_pca)\n",
        "print(\"\\nClassification Report on Full Data:\")\n",
        "print(classification_report(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxXXDPy-Y6V_",
        "outputId": "34b14251-2bb2-43a8-bc8b-093aeae4ddd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 1000\n",
            "PCA-reduced features: 38\n",
            "\n",
            "Cross-validated Accuracy: 0.5600 ¬± 0.0970\n",
            "\n",
            "Classification Report on Full Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.65      0.73        34\n",
            "           1       0.70      0.91      0.79        33\n",
            "           2       0.81      0.76      0.78        33\n",
            "\n",
            "    accuracy                           0.77       100\n",
            "   macro avg       0.78      0.77      0.77       100\n",
            "weighted avg       0.78      0.77      0.77       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IS_qPqgAZLMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}